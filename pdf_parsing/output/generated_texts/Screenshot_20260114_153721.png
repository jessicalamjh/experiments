<x_0.4688><y_0.3>Efficient Exploration for LLMs<x_0.542><y_0.3039><class_Page-header>

<x_0.3389><y_0.318><tbc>Note that our experiment pipeline sidesteps the sort of policy-gradient methods typically used to optimize reward. Instead, our agent samples _N_ responses from the base language model (Gemini Nano) and selects from among those the one that maximizes reward. This best-of-_N_ procedure serves to approximate policy-gradient-based optimization, but without its cumbersome computational requirements. The best-of-_N_ procedure also cultivates more transparent analyses, since it avoids poorly understood dependence on the hyperparameter tinkering often required to obtain reasonable results from policy gradient methods. A prototypical policy gradient approach minimizes a loss function that balances between two objectives: similarity to the base language model and alignment with reward. A scalar hyperparameter multiplies the similarity measure, striking the balance between these objectives. The parameter _N_ plays a similar role in the best-of-_N_ approach. As _N_ increases, maximizing over responses more closely aligns the agent with reward. Moderating _N_ encourages agent behavior more similar to the base language model.<x_0.6738><y_0.4086><class_Text>

<x_0.3389><y_0.4234>**3. Reward Model Architectures and Training**<x_0.5293><y_0.4297><class_Text>

<x_0.3389><y_0.4383>Reward models guide response selection in both the learning and assessment phases of our experiment pipeline. We consider two types of reward models, each of which is fit to observed preference data. The first is a point estimate that assigns a reward to each prompt-response pair. The second depends additionally on an epistemic index. Sampling an epistemic index from a reference distribution induces randomness in reward, which models epistemic uncertainty about the reward. In this section, we describe the neural network architectures and training algorithms used in our experiments.<x_0.6738><y_0.4828><class_Text>

<x_0.3389><y_0.4883>We train reward models that each take as input the last-layer embedding of the Gemini Nano language model. As illustrated in Figure 4, a reward is assigned to a prompt-response pair by first passing it through the language model torso and then through a reward model.<x_0.6729><y_0.5094><class_Text>

<x_0.3389><y_0.6023>### 3.1. Point Estimate<x_0.4082><y_0.607><class_Section-header>

<x_0.3389><y_0.6148>In our architecture, a point estimate reward model takes the form of a feedforward multi-layer perceptron (MLP). This reward model takes as input the last-layer embedding of the Gemini Nano language model, which itself takes as input a prompt-response pair (_x,y_). The reward model then outputs a scalar reward \(\bar{r}_\theta(x,y)\). Here, \(\theta\) is the vector of MLP parameters.<x_0.6729><y_0.6438><class_Text>

<x_0.3389><y_0.65>We train reward models on preference data. Each data point consists of a query, consisting of a prompt and pair of responses, and a binary indication of preference between the responses. Given a set D of such data points, to compute MLP parameters, we optimize the loss function<x_0.6729><y_0.6711><class_Text>

<x_0.4141><y_0.6781>\(\mathcal{L}_\text{point}(\theta|\mathcal{D})=\sum_{(x,y,y^\prime,c)\in\mathcal{D}}\mathrm{ce}(r_\theta(x,y),r_\theta(x,y'),c)+\lambda\left\|\theta\right\|_2^2,\) (1)<x_0.6729><y_0.693><class_Formula>

<x_0.6689><y_0.7172>5<x_0.6729><y_0.7211><class_Page-footer>

<x_0.374><y_0.5172>embedding
token logits
(prompt, response)
torso
unembedding
reward model
reward
epistemic index<x_0.6377><y_0.5656><class_Picture>

<x_0.3389><y_0.5719>Figure 4 | Our reward models take as input the last-layer embedding of the Gemini Nano language model. A stop gradient prevents torso updating of torso weights.<x_0.6719><y_0.5852><class_Caption>
